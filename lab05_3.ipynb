{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ZNKBXASsn5",
        "outputId": "2a3b28b8-9ffb-4d5a-ad86-0d252f376e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Giải nén và loại bỏ thư mục gốc UD_English-EWT/\n",
        "!tar -xzf \"/content/drive/MyDrive/nlp/UD_English-EWT.tar.gz\" -C \"/content/drive/MyDrive/nlp/\" --strip-components=1\n",
        "\n",
        "print(\"GIẢI NÉN XONG! 3 file .conllu nằm NGAY trong MyDrive/nlp:\")\n",
        "!ls -l \"/content/drive/MyDrive/nlp/\" | grep conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFlXxkF5ThPU",
        "outputId": "8eb6bbbd-bb12-4a90-824b-e42e675b05b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIẢI NÉN XONG! 3 file .conllu nằm NGAY trong MyDrive/nlp:\n",
            "-rw------- 1 root root  1756983 Nov  7  2023 en_ewt-ud-dev.conllu\n",
            "-rw------- 1 root root  1758286 Nov  7  2023 en_ewt-ud-test.conllu\n",
            "-rw------- 1 root root 13846707 Nov  7  2023 en_ewt-ud-train.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxVPiIYzTkng",
        "outputId": "080c62f0-6118-442a-890a-c0e2e1a10474"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Tải và Tiền xử lý Dữ liệu"
      ],
      "metadata": {
        "id": "B7Xk6wqMTtxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conllu(path):\n",
        "    sents = []\n",
        "    sent = []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sent: sents.append(sent); sent = []\n",
        "            elif line.startswith('#'):\n",
        "                continue\n",
        "            else:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) >= 10 and parts[0].isdigit():\n",
        "                    word = parts[1].lower()\n",
        "                    tag = parts[3]  # UPOS\n",
        "                    sent.append((word, tag))\n",
        "        if sent: sents.append(sent)\n",
        "    return sents\n",
        "\n",
        "train_sents = load_conllu('/content/drive/MyDrive/nlp/en_ewt-ud-train.conllu')\n",
        "dev_sents   = load_conllu('/content/drive/MyDrive/nlp/en_ewt-ud-dev.conllu')\n",
        "\n",
        "word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
        "tag_to_ix  = {'<PAD>': 0}\n",
        "\n",
        "for sent in train_sents:\n",
        "    for word, tag in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {i: t for t, i in tag_to_ix.items()}\n",
        "\n",
        "print(f\"Train: {len(train_sents)} câu | Dev: {len(dev_sents)} câu\")\n",
        "print(f\"Vocab size: {len(word_to_ix)} | Tagset size: {len(tag_to_ix)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjMA_wFsTvYa",
        "outputId": "04dc3a15-7918-432b-e0be-39a6c810c51e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 12544 câu | Dev: 2001 câu\n",
            "Vocab size: 16656 | Tagset size: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Tạo PyTorch Dataset và DataLoader"
      ],
      "metadata": {
        "id": "pcyvwnmgXPOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSDataset(Dataset):\n",
        "    def __init__(self, data): self.data = data\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        w, t = zip(*self.data[i])\n",
        "        return torch.tensor([word_to_ix.get(x,1) for x in w]), torch.tensor([tag_to_ix[x] for x in t])\n",
        "\n",
        "def collate(b):\n",
        "    x, y = zip(*b)\n",
        "    return pad_sequence(x, batch_first=True, padding_value=0), \\\n",
        "           pad_sequence(y, batch_first=True, padding_value=0)\n",
        "\n",
        "train_loader = DataLoader(POSDataset(train_sents), 32, shuffle=True,  collate_fn=collate)\n",
        "dev_loader   = DataLoader(POSDataset(dev_sents),   32, shuffle=False, collate_fn=collate)"
      ],
      "metadata": {
        "id": "3OGbKAbbXQp4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Xây dựng Mô hình RNN"
      ],
      "metadata": {
        "id": "TVpo8FIyXSIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNForTokenClassification(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, pad_idx):\n",
        "        super().__init__()\n",
        "        # nn.Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # nn.RNN\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        # nn.Linear\n",
        "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HJg2eTTfXTCG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Huấn luyện Mô hình"
      ],
      "metadata": {
        "id": "-YRqLOPxemn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleRNNForTokenClassification(\n",
        "    vocab_size=len(word_to_ix),\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    tagset_size=len(tag_to_ix),\n",
        "    pad_idx=word_to_ix['<PAD>']\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag_to_ix['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"BẮT ĐẦU TASK 4.2 – HUẤN LUYỆN MÔ HÌNH\")\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for i, (sentences, tags) in enumerate(train_loader, 1):\n",
        "        sentences, tags = sentences.to(DEVICE), tags.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()                                   # 1\n",
        "        outputs = model(sentences)                              # 2\n",
        "        loss = criterion(outputs.view(-1, len(tag_to_ix)),      # 3\n",
        "                         tags.view(-1))\n",
        "        loss.backward()                                         # 4\n",
        "        optimizer.step()                                        # 5\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"   Epoch {epoch} | Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"\\nEPOCH {epoch:02d} HOÀN THÀNH – Average Loss: {avg_loss:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eePU7DEJen-i",
        "outputId": "9f19091e-e816-4f81-8f08-84843bf8fa71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BẮT ĐẦU TASK 4.2 – HUẤN LUYỆN MÔ HÌNH\n",
            "   Epoch 1 | Batch 100/392 | Loss: 1.0300\n",
            "   Epoch 1 | Batch 200/392 | Loss: 0.7257\n",
            "   Epoch 1 | Batch 300/392 | Loss: 0.6316\n",
            "\n",
            "EPOCH 01 HOÀN THÀNH – Average Loss: 0.9291\n",
            "\n",
            "   Epoch 2 | Batch 100/392 | Loss: 0.5103\n",
            "   Epoch 2 | Batch 200/392 | Loss: 0.5818\n",
            "   Epoch 2 | Batch 300/392 | Loss: 0.5004\n",
            "\n",
            "EPOCH 02 HOÀN THÀNH – Average Loss: 0.5243\n",
            "\n",
            "   Epoch 3 | Batch 100/392 | Loss: 0.4235\n",
            "   Epoch 3 | Batch 200/392 | Loss: 0.4822\n",
            "   Epoch 3 | Batch 300/392 | Loss: 0.4621\n",
            "\n",
            "EPOCH 03 HOÀN THÀNH – Average Loss: 0.3899\n",
            "\n",
            "   Epoch 4 | Batch 100/392 | Loss: 0.2027\n",
            "   Epoch 4 | Batch 200/392 | Loss: 0.3096\n",
            "   Epoch 4 | Batch 300/392 | Loss: 0.3455\n",
            "\n",
            "EPOCH 04 HOÀN THÀNH – Average Loss: 0.3063\n",
            "\n",
            "   Epoch 5 | Batch 100/392 | Loss: 0.2487\n",
            "   Epoch 5 | Batch 200/392 | Loss: 0.2571\n",
            "   Epoch 5 | Batch 300/392 | Loss: 0.2223\n",
            "\n",
            "EPOCH 05 HOÀN THÀNH – Average Loss: 0.2440\n",
            "\n",
            "   Epoch 6 | Batch 100/392 | Loss: 0.1251\n",
            "   Epoch 6 | Batch 200/392 | Loss: 0.1211\n",
            "   Epoch 6 | Batch 300/392 | Loss: 0.2186\n",
            "\n",
            "EPOCH 06 HOÀN THÀNH – Average Loss: 0.1965\n",
            "\n",
            "   Epoch 7 | Batch 100/392 | Loss: 0.1589\n",
            "   Epoch 7 | Batch 200/392 | Loss: 0.1491\n",
            "   Epoch 7 | Batch 300/392 | Loss: 0.0769\n",
            "\n",
            "EPOCH 07 HOÀN THÀNH – Average Loss: 0.1584\n",
            "\n",
            "   Epoch 8 | Batch 100/392 | Loss: 0.1175\n",
            "   Epoch 8 | Batch 200/392 | Loss: 0.1147\n",
            "   Epoch 8 | Batch 300/392 | Loss: 0.1348\n",
            "\n",
            "EPOCH 08 HOÀN THÀNH – Average Loss: 0.1272\n",
            "\n",
            "   Epoch 9 | Batch 100/392 | Loss: 0.0869\n",
            "   Epoch 9 | Batch 200/392 | Loss: 0.0797\n",
            "   Epoch 9 | Batch 300/392 | Loss: 0.1373\n",
            "\n",
            "EPOCH 09 HOÀN THÀNH – Average Loss: 0.1010\n",
            "\n",
            "   Epoch 10 | Batch 100/392 | Loss: 0.0511\n",
            "   Epoch 10 | Batch 200/392 | Loss: 0.0863\n",
            "   Epoch 10 | Batch 300/392 | Loss: 0.0811\n",
            "\n",
            "EPOCH 10 HOÀN THÀNH – Average Loss: 0.0796\n",
            "\n",
            "TASK 4 HOÀN THÀNH – ĐÃ HUẤN LUYỆN XONG 10 EPOCHS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Đánh giá Mô hình"
      ],
      "metadata": {
        "id": "OQ81PxSXXdja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in loader:\n",
        "            sentences, tags = sentences.to(DEVICE), tags.to(DEVICE)\n",
        "            outputs = model(sentences)\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            mask = tags != tag_to_ix['<PAD>']\n",
        "            correct += (predictions == tags)[mask].sum().item()\n",
        "            total += mask.sum().item()\n",
        "    return correct / total if total > 0 else 0"
      ],
      "metadata": {
        "id": "168DY-OZXe9y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_dev_acc = 0.0\n",
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    for sentences, tags in train_loader:\n",
        "        sentences, tags = sentences.to(DEVICE), tags.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "        loss = criterion(outputs.view(-1, len(tag_to_ix)), tags.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_acc = evaluate(train_loader)\n",
        "    dev_acc   = evaluate(dev_loader)\n",
        "\n",
        "    print(f\"EPOCH {epoch:02d}/10\")\n",
        "    print(f\"   Train Accuracy: {train_acc:.4%}\")\n",
        "    print(f\"   Dev Accuracy  : {dev_acc:.4%}\")\n",
        "\n",
        "    if dev_acc > best_dev_acc:\n",
        "        best_dev_acc = dev_acc\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/nlp/best_model_lab5.pth\")\n",
        "        print(f\"   → ĐÃ LƯU MÔ HÌNH TỐT NHẤT (Dev = {dev_acc:.4%})\")\n",
        "\n",
        "# Hàm predict\n",
        "def predict_sentence(sentence):\n",
        "    model.eval()\n",
        "    words = sentence.lower().split()\n",
        "    ids = [word_to_ix.get(w, word_to_ix['<UNK>']) for w in words]\n",
        "    x = torch.tensor([ids], device=DEVICE)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x).argmax(-1)[0].cpu().tolist()\n",
        "    print(f\"\\nCâu: \\\"{sentence}\\\"\")\n",
        "    for w, p in zip(words, pred):\n",
        "        print(f\"   {w:12} → {ix_to_tag[p]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6TzqRmocS82",
        "outputId": "a9211bf5-0c20-4229-ba24-85bbd64d56e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 01/10\n",
            "   Train Accuracy: 98.7403%\n",
            "   Dev Accuracy  : 88.1148%\n",
            "   → ĐÃ LƯU MÔ HÌNH TỐT NHẤT (Dev = 88.1148%)\n",
            "EPOCH 02/10\n",
            "   Train Accuracy: 99.0160%\n",
            "   Dev Accuracy  : 87.9279%\n",
            "EPOCH 03/10\n",
            "   Train Accuracy: 99.3010%\n",
            "   Dev Accuracy  : 87.8683%\n",
            "EPOCH 04/10\n",
            "   Train Accuracy: 99.3650%\n",
            "   Dev Accuracy  : 87.9439%\n",
            "EPOCH 05/10\n",
            "   Train Accuracy: 99.4907%\n",
            "   Dev Accuracy  : 87.9279%\n",
            "EPOCH 06/10\n",
            "   Train Accuracy: 99.4965%\n",
            "   Dev Accuracy  : 87.5820%\n",
            "EPOCH 07/10\n",
            "   Train Accuracy: 99.5478%\n",
            "   Dev Accuracy  : 87.9081%\n",
            "EPOCH 08/10\n",
            "   Train Accuracy: 99.5674%\n",
            "   Dev Accuracy  : 87.8922%\n",
            "EPOCH 09/10\n",
            "   Train Accuracy: 99.5381%\n",
            "   Dev Accuracy  : 87.9717%\n",
            "EPOCH 10/10\n",
            "   Train Accuracy: 99.3773%\n",
            "   Dev Accuracy  : 87.5621%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"KẾT QUẢ THỰC HIỆN\")\n",
        "print(f\"• Độ chính xác trên tập dev: {best_dev_acc:.4%}\")\n",
        "print(\"\\n• Ví dụ dự đoán câu mới:\")\n",
        "predict_sentence(\"Tuan is very handsome\")\n",
        "predict_sentence(\"My teacher is not as handsome as me\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQjxdCrVhsKn",
        "outputId": "9369fcba-25f9-4d98-9c63-f2bff42b3c2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KẾT QUẢ THỰC HIỆN (copy nguyên vào report)\n",
            "• Độ chính xác trên tập dev: 88.1148%\n",
            "\n",
            "• Ví dụ dự đoán câu mới:\n",
            "\n",
            "Câu: \"Tuan is very handsome\"\n",
            "   tuan         → X\n",
            "   is           → AUX\n",
            "   very         → ADV\n",
            "   handsome     → ADJ\n",
            "\n",
            "Câu: \"My teacher is not as handsome as me\"\n",
            "   my           → PRON\n",
            "   teacher      → NOUN\n",
            "   is           → AUX\n",
            "   not          → PART\n",
            "   as           → ADV\n",
            "   handsome     → ADJ\n",
            "   as           → ADP\n",
            "   me           → PRON\n"
          ]
        }
      ]
    }
  ]
}